# Practice10_ZarsayevYevgeniy

## Отчет по Practice10


## Задание 1. Анализ производительности CPU-параллельной программы (OpenMP)

В этом задании требовалось реализовать параллельную обработку большого массива данных с использованием OpenMP, включая вычисление суммы, среднего значения и дисперсии. Программа была разработана с возможностью изменения числа потоков для выполнения вычислений. Для оценки производительности использовалась функция omp_get_wtime(), что позволило измерить время выполнения отдельных этапов (сумма и дисперсия) и общее время параллельного выполнения. Основная цель — изучить влияние числа потоков на ускорение и оценить долю последовательной и параллельной частей программы в контексте закона Амдала.

### Результаты

<img width="1185" height="139" alt="image" src="https://github.com/user-attachments/assets/ae511bec-788d-4f30-90cc-6cdbe2c611f2" />


Анализ результатов показывает, что при увеличении числа потоков ускорение незначительное и даже иногда наблюдается рост времени выполнения. Например, общее время на 2 потоках составляет 0.0867 с, на 4 потоках — 0.0902 с, а на 8 потоках — 0.0979 с. Это объясняется тем, что последовательная часть программы (инициализация, контроль и синхронизация потоков) ограничивает максимальное ускорение, как предсказывает закон Амдала. Для данной задачи вычислительная нагрузка на каждый элемент невелика, поэтому накладные расходы на управление потоками начинают доминировать, и увеличение числа потоков не приводит к значительному сокращению времени выполнения. Таким образом, для небольших задач оптимальное число потоков ограничено и требует балансировки между параллельной нагрузкой и накладными расходами.

### Блок-схема

<img width="345" height="764" alt="image" src="https://github.com/user-attachments/assets/bbc01598-baab-4abd-a60a-b16098501122" />

## Задание 2. Оптимизация доступа к памяти на GPU (CUDA)

В этом задании требовалось реализовать несколько версий CUDA-ядра для обработки массива данных, демонстрируя влияние различных паттернов доступа к памяти. Были реализованы ядра с коалесцированным и некоалесцированным доступом, а также с оптимизацией через shared memory и изменение организации потоков. Для каждой версии измерялось время выполнения с использованием cudaEvent, что позволило оценить эффективность разных подходов к доступу к глобальной и локальной памяти GPU. Основная цель - понять, как организация памяти и паттерны доступа влияют на производительность.

### Результаты


<img width="416" height="137" alt="image" src="https://github.com/user-attachments/assets/e2ad9151-f1ca-4697-b7dc-766d2bdeec29" />

Анализ результатов показывает сильную зависимость времени выполнения от способа доступа к памяти. Ядро с коалесцированным доступом к глобальной памяти оказалось медленнее (11.0046 мс) по сравнению с версией с некоалесцированным доступом (0.002688 мс) - здесь, вероятно, эффект связан с небольшим объёмом данных и оптимизацией компилятора. Использование shared memory и изменение организации потоков дополнительно сократило время до 0.002496 и 0.002432 мс соответственно, демонстрируя, что эффективное использование локальной памяти и правильная организация потоков могут значительно уменьшить накладные расходы доступа к глобальной памяти и повысить производительность GPU. Таким образом, для задач с интенсивной работой с памятью критически важно выбирать подходящий паттерн доступа и использовать локальные буферы.


### Блок-схема

<img width="395" height="800" alt="image" src="https://github.com/user-attachments/assets/981ee6ef-c7d1-46fb-bd72-80b4ac32ac2f" />


## Task3


### Результаты


<img width="464" height="285" alt="image" src="https://github.com/user-attachments/assets/70b9e4ae-fc87-4e4d-b8e1-4f527bf19123" />


### Блок-схема

<img width="345" height="802" alt="image" src="https://github.com/user-attachments/assets/83c387b2-5e36-4b8b-81c7-ca2a91a6d593" />


## Task4


### Результаты


<img width="592" height="420" alt="image" src="https://github.com/user-attachments/assets/21993c57-8fe3-41c0-a6b5-0b7e4f658d2e" />


### Блок-схема

<img width="274" height="834" alt="image" src="https://github.com/user-attachments/assets/44d520a3-b176-4e2b-8ce9-aebab15d0e3b" />



# Контрольные вопросы


1. В чём отличие измерения времени выполнения от профилирования?


Ответ: Измерение времени выполнения показывает, сколько времени программа работает в целом или на отдельных участках. Профилирование идёт глубже и показывает, где именно тратится время: какие функции самые медленные и где возникают задержки.


2. Какие виды узких мест характерны для CPU, GPU и распределённых программ?


Ответ: Для CPU это может быть ограниченное количество ядер, частые ветвления и ожидание памяти. Для GPU - медленный доступ к памяти, плохая загрузка потоков и лишние копирования данных. В распределённых программах узким местом чаще всего становится обмен данными между узлами и задержки сети.


3. Почему увеличение числа потоков или процессов не всегда приводит к ускорению?
   

Ответ: Потому что растут накладные расходы на создание, синхронизацию и обмен данными. Кроме того, всегда есть часть программы, которая выполняется последовательно и не может быть распараллелена.


4. Как законы Амдала и Густафсона применяются при анализе масштабируемости?
   

Ответ: Закон Амдала показывает, что ускорение ограничено долей последовательного кода и бесконечного ускорения добиться нельзя. Закон Густафсона говорит, что при увеличении размера задачи параллельная часть растёт, и масштабируемость может быть лучше на больших данных.


5. Какие факторы наиболее критичны для производительности гибридных приложений?


Ответ: Критичны правильное распределение работы между CPU и GPU, минимизация передачи данных между ними и эффективное использование памяти. Также важно учитывать баланс нагрузки, чтобы ни CPU, ни GPU не простаивали.


### Сборка

!g++ -fopenmp task1_openmp.cpp -o task1_openmp

!nvcc task2_cuda.cu -o task2_cuda

!nvcc task3_hybrid.cu -o task3_hybrid

!mpicxx task4_mpi.cpp -o task4_mpi

### Запуск

!./task1_openmp

!./task2_cuda

!./task3_hybrid

!mpirun --allow-run-as-root --oversubscribe -np 2 ./task4_mpi
