### Assignment3_ZarsayevYevgeniy

## Отчет по работе с CUDA
В этот раз сильно постарался с оформлением, надеюсь все сделал по стандартам.

В этой работе я выполнил 4 задания, каждое из которых связано с программированием на GPU с использованием CUDA. Основная цель была познакомиться с параллельной обработкой массивов, измерением производительности разных вариантов и оптимизацией параметров блоков и сетки потоков. В процессе работы я создавал ядра для глобальной и shared памяти, экспериментировал с размерами блоков и проверял, как коалесцированный и некоалесцированный доступ к памяти влияет на скорость выполнения программ. Все задания были запущены на GPU с помощью Google Colab, а результаты я измерял с помощью встроенных таймеров CUDA.

# Задание 1 – Поэлементное умножение массива
В первом задании требовалось реализовать программу, которая умножает каждый элемент массива на число. Нужно было сделать две версии: с использованием глобальной памяти и с использованием shared памяти.

Я получил следующие результаты:

<img width="413" height="121" alt="image" src="https://github.com/user-attachments/assets/d15b5741-c018-49eb-bbbb-75fbf44fb3b0" />

Это показывает, что оба варианта работают корректно, но shared память работает почти в два раза быстрее, что подтверждает, что использование быстрой памяти ускоряет обработку данных на GPU.

Вот блок схема для этого задания:

<img width="433" height="833" alt="image" src="https://github.com/user-attachments/assets/913750fe-5667-42fa-ac9f-fbce94421f68" />

# Задание 2 – Сложение массивов и влияние размера блока
Во втором задании нужно было сложить два массива и исследовать, как размер блока потоков влияет на производительность программы. Я проверял три размера блока: 128, 256 и 512 потоков.

Результаты:

<img width="525" height="103" alt="image" src="https://github.com/user-attachments/assets/d77ace43-bca0-4977-bb67-b9479db95262" />

Все блоки дают правильный результат, но при увеличении блока до 256 потоков скорость выполнения сильно улучшается. Блок размером 512 потоков работает почти так же быстро, как 256, что говорит о том, что дальше увеличение размера блока не даёт значительного прироста.

Блок схема:

<img width="420" height="808" alt="image" src="https://github.com/user-attachments/assets/a3b2855e-bdb4-4735-9f73-f3be9c3691e7" />

# Задание 3 – Коалесцированный и некоалесцированный доступ
В третьем задании нужно было сравнить, как скорость работы меняется при последовательном коалесцированном и «разбросанном», то есть некоалесцированном доступе к глобальной памяти.

Результаты:

<img width="455" height="87" alt="image" src="https://github.com/user-attachments/assets/8d598eaf-398e-4895-8253-2563972fdab5" />

Оба варианта дают правильный результат (2), но при некоалесцированном доступе время выполнения сильно увеличивается (почти в 50 раз). Это наглядно показывает, насколько важно правильно организовывать доступ к памяти на GPU для ускорения программ.

Блок схема:

<img width="359" height="817" alt="image" src="https://github.com/user-attachments/assets/6ecded86-6980-4b50-8b11-639d648629ba" />

# Задание 4 – Оптимизация конфигурации блоков
В четвертом задании нужно было подобрать оптимальные параметры сетки и блоков потоков для одного из предыдущих заданий и сравнить с неоптимальной конфигурацией.

Результаты:

<img width="574" height="90" alt="image" src="https://github.com/user-attachments/assets/8e1f02e2-2752-40c6-8cca-3d461a0336c2" />

Блок схема:

<img width="308" height="761" alt="image" src="https://github.com/user-attachments/assets/1eacf80f-5dde-4fe3-acf0-2a8090cd8b7f" />

## Вывод
В ходе работы я убедился, что параллельная обработка на GPU даёт значительное ускорение, особенно при правильном использовании shared памяти и оптимальной конфигурации блоков. Также я увидел, как критично влияет способ доступа к памяти (коалесцированный или нет) на производительность. Все четыре задания помогли мне лучше понять принципы работы CUDA и важность оптимизации для ускорения вычислений.


### Сборка
!nvcc task.cu -O2 -gencode arch=compute_75,code=sm_75 -o task

### Запуск
!./task
